\section{Discussion}
\subsection{Implications for Human–AI Interaction Design}
The findings of this study provide several preliminary implications for the design of human-centered AI learning systems. Even before final statistical results are fully interpreted, the overall structure of the experiment highlights the importance of \textbf{interaction style} as a key design dimension in AI-supported learning environments.
\\
First, the comparison between empathic and neutral avatar modes suggests that \textbf{emotional tone and feedback style} should be treated as explicit design variables rather than incidental characteristics of AI tutors. In Human–AI Interaction, this supports the idea that conversational agents are not only information providers but also social actors whose communication style can shape users’ learning experience.
\\
Second, the use of an avatar-based conversational interface emphasizes the role of \textbf{social presence} in learning contexts. Even without voice interaction, the combination of visual embodiment and adaptive textual feedback may influence how learners perceive support, clarity, and engagement. This underlines the importance of designing AI tutors that balance instructional clarity with socially appropriate responses.
\\
Finally, the study highlights the need for \textbf{integrated evaluation frameworks} in HAI research that jointly consider cognitive outcomes (e.g., perceived learning effectiveness) and affective outcomes (e.g., empathy, trust, engagement). Designing AI learning systems with these dimensions in mind may lead to more sustainable and user-aligned educational technologies.
\\
These implications will be further refined once the final results are analyzed, allowing a more precise mapping between observed effects and concrete design recommendations.

\subsection{Limitations}
This study has several limitations that should be considered when interpreting the results.
\\
First, the \textbf{sample size} was relatively small, which limits the generalizability of the findings. Although the study aimed to recruit at least 30 participants, inconsistencies across pre-survey, interaction, and post-survey data reduced the number of fully matched cases available for analysis.
\\
Second, the study faced \textbf{data consistency and matching challenges}. Some participants used inconsistent or missing IDs across different phases of the experiment, making it difficult to reliably link pre-survey, interaction logs, and post-survey responses. While data cleaning procedures were applied to retain the most reliable matches, this issue reduced the usable dataset.
\\
Third, \textbf{temporal inconsistencies} were observed in some cases, where survey completion times did not logically align with interaction timestamps. Due to time constraints and the proximity of the deadline, these cases could not be fully resolved and were handled using best-effort matching strategies.
\\
Fourth, the study relied exclusively on \textbf{self-reported measures} for perceived learning effectiveness, empathy, and engagement. While these measures are common in HAI research, they may be influenced by subjective bias and do not directly capture objective learning performance.
\\
Finally, although the system initially included voice interaction, this feature was removed based on early user feedback indicating distraction and discomfort. As a result, the study does not assess the potential impact of multimodal interaction (e.g., speech) on learning outcomes, which may limit the scope of design insights.
\\
Despite these limitations, the study provides valuable exploratory insights into the role of AI avatar personality in learning contexts and offers a foundation for more controlled and scalable future investigations.

